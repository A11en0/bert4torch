{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 计算向量，统计结果等"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. 获得基于第二阶段模型所得标问向量"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/tf/libo/Project/im/exe')\n",
    "from task_sentence_embedding_FinanceFAQ_step2_1 import model\n",
    "from config import *\n",
    "\n",
    "#get list\n",
    "std_data=pd.read_csv(std_data_path, sep=\"\\t\")\n",
    "q_std_list=std_data.drop_duplicates(\"q_std\").q_std.tolist()\n",
    "q_sim_list=std_data.drop_duplicates(\"q_sim\").q_sim.tolist()\n",
    "q_corpus=pd.DataFrame(q_std_list+q_sim_list,columns=['q']).drop_duplicates(\"q\").q.tolist()\n",
    "q_std_df=pd.DataFrame(q_std_list,columns=['q'])\n",
    "q_corpus_df=pd.DataFrame(q_corpus,columns=['q'])\n",
    "q_sim_df=pd.DataFrame(q_sim_list,columns=['q'])\n",
    "print('保存标问：', q_std_df.shape, os.path.join(data_dir,sec_q_std_file))\n",
    "q_std_df.to_csv(os.path.join(data_dir,sec_q_std_file),index=None,header=False,sep=\"\\t\")\n",
    "print('保存所有语料：', q_corpus_df.shape, os.path.join(data_dir,sec_q_corpus_file))\n",
    "q_corpus_df.to_csv(os.path.join(data_dir,sec_q_corpus_file),index=None,header=False,sep=\"\\t\")\n",
    "print('保存相似问：', q_sim_df.shape, os.path.join(data_dir,sec_q_sim_file))\n",
    "q_sim_df.to_csv(os.path.join(data_dir,sec_q_sim_file),index=None,header=False,sep=\"\\t\")\n",
    "print('q_std_list:——>',len(q_std_list),'q_sim_list:——>',len(q_sim_list),'q_corpus:——>',len(q_corpus))\n",
    "#get embeddings\n",
    "q_std_sentence_embeddings = model.encode(q_std_list,batch_size=64)\n",
    "print('保存标准问向量：', os.path.join(data_dir,sec_q_std_vectors_file))\n",
    "np.save(os.path.join(data_dir,sec_q_std_vectors_file),q_std_sentence_embeddings)\n",
    "q_corpus_sentence_embeddings = model.encode(q_corpus,batch_size=64)\n",
    "print('保存所有语料向量：', os.path.join(data_dir,sec_q_corpus_vectors_file))\n",
    "np.save(os.path.join(data_dir,sec_q_corpus_vectors_file),q_corpus_sentence_embeddings)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. 获得所有待测数据第一阶段模型预测结果"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import sys\n",
    "sys.path.append('/tf/libo/Project/im/exe')\n",
    "from my_sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "import numpy as np\n",
    "import logging\n",
    "import pandas as pd\n",
    "from config_train import *\n",
    "\n",
    "print('*'*200,'eva_s1')\n",
    "print('***********currentD:',currentD)\n",
    "path_list=path_list_1\n",
    "model_dir=fst_model_dir\n",
    "data_dir=fst_data_dir\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "def pytorch_cos_sim(a: Tensor, b: Tensor):\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = a / a.norm(dim=1)[:, None]\n",
    "    b_norm = b / b.norm(dim=1)[:, None]\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "print('**load model from',model_dir)\n",
    "##\n",
    "def cos_sim(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    计算两个向量之间的余弦相似度\n",
    "    :param vector_a: 向量 a\n",
    "    :param vector_b: 向量 b\n",
    "    :return: sim\n",
    "    \"\"\"\n",
    "    vector_a = np.mat(vector_a)\n",
    "    vector_b = np.mat(vector_b)\n",
    "    num = float(vector_a * vector_b.T)\n",
    "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    return sim\n",
    "\n",
    "def cos_sim_1(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    计算两个向量之间的余弦相似度\n",
    "    :param vector_a: 向量 a\n",
    "    :param vector_b: 向量 b\n",
    "    :return: sim\n",
    "    \"\"\"\n",
    "    vector_a = np.mat(vector_a)\n",
    "    vector_b = np.mat(vector_b)\n",
    "    num = float(vector_a * vector_b.T)\n",
    "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "    cos = num / denom\n",
    "    return cos\n",
    "\n",
    "def cos_sim4matrix(arr, brr):\n",
    "    return 0.5 + 0.5 * (arr.dot(brr.T) / (np.sqrt(np.sum(arr * arr)) * np.sqrt(np.sum(brr * brr, axis = 1))))\n",
    "\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "np.set_printoptions(threshold=100)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "\n",
    "# 加载模型\n",
    "print('模型地址', model_dir)\n",
    "model = SentenceTransformer(model_dir)\n",
    "\n",
    "\n",
    "print('读取标准问及其向量'.center(60, '-'))\n",
    "q_std_list=pd.read_csv(os.path.join(data_dir,fst_q_std_file),sep=\"\\t\",names=['c']).c.tolist()\n",
    "q_std_sentence_embeddings=np.load(os.path.join(data_dir,fst_q_std_vectors_file))\n",
    "print('标准问数量：——>',q_std_sentence_embeddings.shape,len(q_std_list))\n",
    "\n",
    "print('读取所有语料及其向量'.center(60, '-'))\n",
    "q_all=pd.read_csv(os.path.join(data_dir,fst_q_corpus_file),sep=\"\\t\",names=['c']).c.tolist()\n",
    "q_all_sentence_embeddings=np.load(os.path.join(data_dir,fst_q_corpus_vectors_file))\n",
    "q_all_sentence_embeddings_dict={q_all[i]:q_all_sentence_embeddings[i] for i in range(0,len(q_all))}\n",
    "print('所有问句数量：——>',q_all_sentence_embeddings.shape,len(q_all))\n",
    "# # 记得选择是v1还是v2\n",
    "# # v1\n",
    "q_corpus=q_std_list\n",
    "corpus_sentence_embeddings=q_std_sentence_embeddings\n",
    "dict_2={v:v for v in q_std_list}\n",
    "pred2std_dict=dict_2\n",
    "len(pred2std_dict)\n",
    "\n",
    "\n",
    "for i in range(0,len(path_list)):\n",
    "    print(f'开始评估新语料: {i}'.center(120, '='))\n",
    "    df_k=pd.read_csv(path_list[i][0],sep=\"\\t\")\n",
    "    df_k=df_k[~pd.isna(df_k.q_sim)]\n",
    "    outputpath=path_list[i][1]\n",
    "    print(path_list[i][0])\n",
    "    print(outputpath)\n",
    "\n",
    "    df_k['ifin']=df_k.q_std.apply(lambda v:1 if v in q_std_list  else 0)\n",
    "    print(\"目标语料标问是否存在：——>\",df_k.groupby(\"ifin\")[\"ifin\"].count())\n",
    "    print(\"目标语料数量：\",df_k.shape,'标问数量：',df_k.drop_duplicates(\"q_std\").shape[0],'相似问数量：',df_k.drop_duplicates(\"q_sim\").shape[0],'标语料去重后数量',df_k.drop_duplicates([\"q_std\",\"q_sim\"]).shape[0])\n",
    "    texts=df_k.q_sim.tolist()\n",
    "    texts_in=[v for v in texts if v in q_all_sentence_embeddings_dict.keys()]\n",
    "    texts_out=[v for v in texts if v not in q_all_sentence_embeddings_dict.keys()]\n",
    "    texts_out_embeddings=model.encode(texts_out,batch_size=64)\n",
    "    texts_embeddings_dict_1={texts_in[i]:q_all_sentence_embeddings_dict[texts_in[i]] for i in range(0,len(texts_in))}\n",
    "    texts_embeddings_dict_2={texts_out[i]:texts_out_embeddings[i]  for i in range(0,len(texts_out))}\n",
    "    texts_embeddings_dict={**texts_embeddings_dict_1, **texts_embeddings_dict_2}\n",
    "    print('目标语料编码后数量：——>',len(texts_embeddings_dict))\n",
    "\n",
    "    ## v1 对于都是有一个是小量的情况下\n",
    "    K=10\n",
    "    x_texts=texts\n",
    "    x_texts_embeddings=np.array([texts_embeddings_dict[x_text] for x_text in x_texts])\n",
    "    cos_scores = pytorch_cos_sim(x_texts_embeddings,corpus_sentence_embeddings).cpu()\n",
    "    cos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(cos_scores, K, dim=1, largest=True, sorted=False)\n",
    "    cos_scores_top_k_values = cos_scores_top_k_values.tolist()\n",
    "    cos_scores_top_k_idx = cos_scores_top_k_idx.tolist()\n",
    "    cos_q_corpus_sort=[[q_corpus[v] for v in vlist] for vlist in cos_scores_top_k_idx ]\n",
    "    result=[list(zip(cos_q_corpus_sort[i],cos_scores_top_k_values[i])) for i in range(0,len(x_texts))]\n",
    "    texts_topk_dict={texts[i]:result[i] for i in range(0,len(texts))}\n",
    "    #\n",
    "    df_k['q_std_pred_list']=df_k.q_sim.map(texts_topk_dict)\n",
    "    df_k.loc[:,'q_std_pred']=df_k.q_std_pred_list.apply(lambda v:v[0][0])\n",
    "    df_k.loc[:,'prob']=df_k.q_std_pred_list.apply(lambda v:v[0][1])\n",
    "    # df_k.loc[:,'q_std_pred_list_pair']=df_k.apply(lambda row: [(row['q_std'],row['q_sim'],v[0],v[1]) for v in row['q_std_pred_list']],axis=1)\n",
    "    df_k['q_std_pred_list_v1']=df_k.q_std_pred_list.apply(lambda v:[k[0] for k in v])\n",
    "    df_k['q_std_pred_list_v2']=df_k.q_std_pred_list.apply(lambda v:[k[1] for k in v])\n",
    "    df_k['t1']=df_k.apply(lambda row:1 if row['q_std'] in row['q_std_pred_list_v1'][0:1] else 0,axis=1)\n",
    "    df_k['t3']=df_k.apply(lambda row:1 if row['q_std'] in row['q_std_pred_list_v1'][0:3] else 0,axis=1)\n",
    "    df_k['t5']=df_k.apply(lambda row:1 if row['q_std'] in row['q_std_pred_list_v1'][0:5] else 0,axis=1)\n",
    "    df_k['t10']=df_k.apply(lambda row:1 if row['q_std'] in row['q_std_pred_list_v1'][0:10] else 0,axis=1)\n",
    "\n",
    "    print('目标语料准确率：——>')\n",
    "    print(df_k.shape)\n",
    "    print(df_k.t1.sum()/df_k.shape[0],df_k.t3.sum()/df_k.shape[0],df_k.t5.sum()/df_k.shape[0],df_k.t10.sum()/df_k.shape[0])\n",
    "    print('目标语料准确率[有效标问]：——>')\n",
    "    df_k_need=df_k[df_k.ifin==1]\n",
    "    print(df_k_need.shape)\n",
    "    print(df_k_need.t1.sum()/df_k_need.shape[0],df_k_need.t3.sum()/df_k_need.shape[0],df_k_need.t5.sum()/df_k_need.shape[0],df_k_need.t10.sum()/df_k_need.shape[0])\n",
    "\n",
    "    print(outputpath)\n",
    "\n",
    "    df_k.to_csv(outputpath,index=None,sep=\"\\t\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 获得所有待测数据第二阶段模型预测结果"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "from task_sentence_embedding_FinanceFAQ_step2_1 import model\n",
    "import numpy as np\n",
    "import logging\n",
    "import pandas as pd\n",
    "from config import *\n",
    "\n",
    "path_list=path_list_2\n",
    "model_dir=sec_model_dir\n",
    "data_dir=sec_data_dir\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "def pytorch_cos_sim(a: Tensor, b: Tensor):\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = a / a.norm(dim=1)[:, None]\n",
    "    b_norm = b / b.norm(dim=1)[:, None]\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "print('**load model from',model_dir)\n",
    "##\n",
    "def cos_sim(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    计算两个向量之间的余弦相似度\n",
    "    :param vector_a: 向量 a\n",
    "    :param vector_b: 向量 b\n",
    "    :return: sim\n",
    "    \"\"\"\n",
    "    vector_a = np.mat(vector_a)\n",
    "    vector_b = np.mat(vector_b)\n",
    "    num = float(vector_a * vector_b.T)\n",
    "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    return sim\n",
    "\n",
    "def cos_sim_1(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    计算两个向量之间的余弦相似度\n",
    "    :param vector_a: 向量 a\n",
    "    :param vector_b: 向量 b\n",
    "    :return: sim\n",
    "    \"\"\"\n",
    "    vector_a = np.mat(vector_a)\n",
    "    vector_b = np.mat(vector_b)\n",
    "    num = float(vector_a * vector_b.T)\n",
    "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "    cos = num / denom\n",
    "    return cos\n",
    "\n",
    "def cos_sim4matrix(arr, brr):\n",
    "    return 0.5 + 0.5 * (arr.dot(brr.T) / (np.sqrt(np.sum(arr * arr)) * np.sqrt(np.sum(brr * brr, axis = 1))))\n",
    "\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "np.set_printoptions(threshold=100)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "\n",
    "# 加载模型\n",
    "model = SentenceTransformer(model_dir)\n",
    "\n",
    "q_std_list=pd.read_csv(os.path.join(data_dir,sec_q_std_file),sep=\"\\t\",names=['c']).c.tolist()\n",
    "q_std_sentence_embeddings=np.load(os.path.join(data_dir,sec_q_std_vectors_file))\n",
    "print('标问数量：——>',q_std_sentence_embeddings.shape,len(q_std_list))\n",
    "q_all=pd.read_csv(os.path.join(data_dir,sec_q_corpus_file),sep=\"\\t\",names=['c']).c.tolist()\n",
    "q_all_sentence_embeddings=np.load(os.path.join(data_dir,sec_q_corpus_vectors_file))\n",
    "q_all_sentence_embeddings_dict={q_all[i]:q_all_sentence_embeddings[i] for i in range(0,len(q_all))}\n",
    "print('所有问句数量：——>',q_all_sentence_embeddings.shape,len(q_all))\n",
    "# # 记得选择是v1还是v2\n",
    "# # v1\n",
    "q_corpus=q_std_list\n",
    "corpus_sentence_embeddings=q_std_sentence_embeddings\n",
    "corpus_sentence_embeddings_dict={q_corpus[i]:corpus_sentence_embeddings[i] for i in range(0,len(q_corpus))}\n",
    "dict_2={v:v for v in q_std_list}\n",
    "pred2std_dict=dict_2\n",
    "\n",
    "\n",
    "for i in range(0,len(path_list)):  \n",
    "    print(f'开始评估新语料: {i}'.center(120, '='))\n",
    "    # # # ##v1\n",
    "    df_k=pd.read_csv(path_list[i][0],sep=\"\\t\")\n",
    "    outputpath=path_list[i][1]\n",
    "    print(path_list[i][0])\n",
    "    print(outputpath)\n",
    "    \n",
    "    texts=df_k.q_sim.tolist()\n",
    "    texts_in=[v for v in texts if v in q_all_sentence_embeddings_dict.keys()]\n",
    "    texts_out=[v for v in texts if v not in q_all_sentence_embeddings_dict.keys()]\n",
    "    texts_out_embeddings=model.encode(texts_out,batch_size=64)\n",
    "    texts_embeddings_dict_1={texts_in[i]:q_all_sentence_embeddings_dict[texts_in[i]] for i in range(0,len(texts_in))}\n",
    "    texts_embeddings_dict_2={texts_out[i]:texts_out_embeddings[i]  for i in range(0,len(texts_out))}\n",
    "    texts_embeddings_dict={**texts_embeddings_dict_1, **texts_embeddings_dict_2}\n",
    "    print('目标语料编码数量：——>',len(texts_embeddings_dict))\n",
    "\n",
    "    def cos_sim4matrix_2(arr, brr):\n",
    "        return (arr.dot(brr.T) / (np.sqrt(np.sum(arr * arr)) * np.sqrt(np.sum(brr * brr, axis = 1))))\n",
    "\n",
    "    def get_sec_result(text,std_texts):\n",
    "        a_text_embeddings=texts_embeddings_dict[text]\n",
    "        b_text_embeddings=np.array([corpus_sentence_embeddings_dict[v] for v in std_texts])\n",
    "    #     print(a_text_embeddings.shape,b_text_embeddings.shape)\n",
    "        sims_with_std=cos_sim4matrix_2(a_text_embeddings,b_text_embeddings).reshape(-1)\n",
    "        sort_idx=np.argsort(-sims_with_std).tolist()\n",
    "        intents_sort=[std_texts[idx] for idx in sort_idx]\n",
    "        sims_values=[sims_with_std[idx] for idx in sort_idx]\n",
    "        result=list(zip(intents_sort,sims_values))\n",
    "        return(result)\n",
    "    # get_sec_result(text,std_texts)\n",
    "\n",
    "    df_k['q_std_pred_list_v1']=df_k.q_std_pred_list_v1.apply(lambda v:eval(v))\n",
    "\n",
    "    df_k['q_std_pred_list_2']=df_k.apply(lambda row:get_sec_result(row['q_sim'],row['q_std_pred_list_v1']),axis=1)\n",
    "\n",
    "\n",
    "    df_k['q_std_pred_list_2_v1']=df_k.q_std_pred_list_2.apply(lambda v:[k[0] for k in v])\n",
    "    df_k['q_std_pred_list_2_v2']=df_k.q_std_pred_list_2.apply(lambda v:[k[1] for k in v])\n",
    "    df_k['q_std_pred_2']=df_k.q_std_pred_list_2_v1.apply(lambda v:v[0])\n",
    "    df_k['prob_2']=df_k.q_std_pred_list_2_v2.apply(lambda v:v[0])\n",
    "\n",
    "\n",
    "    df_k['r1']=df_k.apply(lambda row:1 if row['q_std'] in row['q_std_pred_list_2_v1'][0:1] else 0,axis=1)\n",
    "    df_k['r3']=df_k.apply(lambda row:1 if row['q_std'] in row['q_std_pred_list_2_v1'][0:3] else 0,axis=1)\n",
    "    df_k['r5']=df_k.apply(lambda row:1 if row['q_std'] in row['q_std_pred_list_2_v1'][0:5] else 0,axis=1)\n",
    "    df_k['r10']=df_k.apply(lambda row:1 if row['q_std'] in row['q_std_pred_list_2_v1'][0:10] else 0,axis=1)\n",
    "\n",
    "\n",
    "    ##扣除不包含的标准问\n",
    "    print('目标语料准确率：——>')\n",
    "    print(df_k.shape)\n",
    "    df_1=df_k\n",
    "    print('第一阶段整体准确率',df_1.t1.sum()/df_1.shape[0],df_1.t3.sum()/df_1.shape[0],df_1.t5.sum()/df_1.shape[0],df_1.t10.sum()/df_1.shape[0])\n",
    "    df_2=df_k[df_k.t10==1]\n",
    "    print('第二阶段整体准确率',df_2.r1.sum()/df_2.shape[0],df_2.r3.sum()/df_2.shape[0],df_2.r5.sum()/df_2.shape[0],df_2.r10.sum()/df_2.shape[0])\n",
    "    df_3=df_k\n",
    "    print('整体准确率',df_3.r1.sum()/df_3.shape[0],df_3.r3.sum()/df_3.shape[0],df_3.r5.sum()/df_3.shape[0],df_3.r10.sum()/df_3.shape[0])\n",
    "\n",
    "\n",
    "    ##扣除不包含的标准问\n",
    "    print('目标语料准确率[有效标问]：——>')\n",
    "    df_k_need=df_k[df_k.ifin==1]\n",
    "    print(df_k_need.shape)\n",
    "    df_1=df_k_need\n",
    "    print('第一阶段整体准确率',df_1.t1.sum()/df_1.shape[0],df_1.t3.sum()/df_1.shape[0],df_1.t5.sum()/df_1.shape[0],df_1.t10.sum()/df_1.shape[0])\n",
    "    df_2=df_k_need[df_k_need.t10==1]\n",
    "    print('第二阶段整体准确率',df_2.r1.sum()/df_2.shape[0],df_2.r3.sum()/df_2.shape[0],df_2.r5.sum()/df_2.shape[0],df_2.r10.sum()/df_2.shape[0])\n",
    "    df_3=df_k_need\n",
    "    print('整体准确率',df_3.r1.sum()/df_3.shape[0],df_3.r3.sum()/df_3.shape[0],df_3.r5.sum()/df_3.shape[0],df_3.r10.sum()/df_3.shape[0])\n",
    "\n",
    "    print(outputpath)\n",
    "    df_k.to_csv(outputpath,index=None,sep=\"\\t\")\n",
    "\t\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 获得单例文本预测结果"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/tf/libo/Project/im/exe')\n",
    "from my_sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "# from my_sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "import config_train as config\n",
    "\n",
    "print('***********currentD:',config.currentD)\n",
    "\n",
    "# currentD='20210805'\n",
    "text=\"我想开户\"\n",
    "# # v1\n",
    "step1_model_dir=\"/tf/yy/im_all_0823/tm/output/%s_fst_d1/m2/\"%config.currentD\n",
    "step1_data_dir=\"/tf/yy/im_all_0823/tm/data_train/%s_fst_d1\"%config.currentD\n",
    "step2_model_dir=\"/tf/yy/im_all_0823/tm/output/%s_sec_d1/m2/\"%config.currentD\n",
    "step2_data_dir=\"/tf/yy/im_all_0823/tm/data_train/%s_sec_d1\"%config.currentD\n",
    "# # # v2\n",
    "# step1_model_dir=\"/tf/yy/prd/%s/topk_model_info_1/model_dir\"%config.currentD\n",
    "# step1_data_dir=\"/tf/yy/prd/%s/topk_model_info_1\"%config.currentD\n",
    "# step2_model_dir=\"/tf/yy/prd/%s/topk_model_info_2/model_dir\"%config.currentD\n",
    "# step2_data_dir=\"/tf/yy/prd/%s/topk_model_info_2\"%config.currentD\n",
    "\n",
    "def set_model_gpu(setgpu):\n",
    "    if setgpu==True:\n",
    "        print('open GPU')\n",
    "        os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')\n",
    "        memory_gpu=[int(x.split()[2]) for x in open('tmp','r').readlines()]\n",
    "        gpu_n=str(np.argmax(memory_gpu))\n",
    "        os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "        print(\"use gpu %s\"%gpu_n)\n",
    "#         memory_gpu=[]\n",
    "    else:\n",
    "        print('shut down GPU')\n",
    "        os.environ['CUDA_VISIBLE_DEVICES']='-1'\n",
    "    os.system('rm tmp')\n",
    "\n",
    "\n",
    "    \n",
    "class get_sim_first():\n",
    "    def __init__(self,step1_model_dir,step1_data_dir,step1_q_std_vector_filename):\n",
    "        self.model_dir=step1_model_dir\n",
    "        self.data_dir=step1_data_dir\n",
    "        self.q_std_filename=config.fst_q_std_file\n",
    "        self.q_std_vector_filename=step1_q_std_vector_filename\n",
    "        self.q_std_path=os.path.join(self.data_dir,self.q_std_filename)\n",
    "        ## set gpu\n",
    "        \n",
    "        ## load std questions\n",
    "        with open(self.q_std_path,'r',encoding=\"utf-8\") as f:\n",
    "            data=f.readlines()\n",
    "        self.q_std_list=[v.strip() for v in data]\n",
    "        ## load std questions' vectors\n",
    "        self.std_sentence_embeddings=np.load(os.path.join(self.data_dir,self.q_std_vector_filename))\n",
    "        ## load model\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = SentenceTransformer(self.model_dir,device=device)\n",
    "        ## \n",
    "        \n",
    "    def cos_sim(self,vector_a,vector_b):\n",
    "        vector_a = np.mat(vector_a)\n",
    "        vector_b = np.mat(vector_b)\n",
    "        num = float(vector_a * vector_b.T)\n",
    "        denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "        cos = num / denom\n",
    "        sim = 0.5 + 0.5 * cos\n",
    "        return sim\n",
    "    \n",
    "    def cos_sim4matrix(self,arr, brr):\n",
    "        return 0.5 + 0.5 * (arr.dot(brr.T) / (np.sqrt(np.sum(arr * arr)) * np.sqrt(np.sum(brr * brr, axis = 1))))\n",
    "    def cos_sim4matrix_2(self,arr, brr):\n",
    "        return (arr.dot(brr.T) / (np.sqrt(np.sum(arr * arr)) * np.sqrt(np.sum(brr * brr, axis = 1))))\n",
    "    \n",
    "    def get_topK(self,text,K=20):\n",
    "        text_embedding=self.model.encode([text])[0]\n",
    "#         sims_with_std=np.array([self.cos_sim(vec,text_embedding) for vec in self.std_sentence_embeddings])\n",
    "        sims_with_std=np.array(self.cos_sim4matrix_2(text_embedding,self.std_sentence_embeddings))\n",
    "        sort_idx=np.argsort(-sims_with_std)[:K]\n",
    "        sims_q_sort=[self.q_std_list[idx] for idx in sort_idx]\n",
    "        sims_values=[sims_with_std[idx] for idx in sort_idx]\n",
    "        result=list(zip(sims_q_sort,sims_values))\n",
    "        return(result)\n",
    "  \n",
    "    \n",
    "class get_sim_second():\n",
    "    def __init__(self,step2_model_dir,step2_data_dir,step2_q_std_vector_filename):\n",
    "        self.model_dir=step2_model_dir\n",
    "        self.data_dir=step2_data_dir\n",
    "        self.q_std_filename=config.sec_q_std_file\n",
    "        self.q_std_vector_filename=step2_q_std_vector_filename\n",
    "        self.q_std_path=os.path.join(self.data_dir,self.q_std_filename)\n",
    "        \n",
    "        ## set gpu\n",
    "        \n",
    "        ## load std questions\n",
    "        with open(self.q_std_path,'r',encoding=\"utf-8\") as f:\n",
    "            data=f.readlines()\n",
    "        self.q_std_list=[v.strip() for v in data]\n",
    "        ## load std questions' vectors\n",
    "        self.std_sentence_embeddings=np.load(os.path.join(self.data_dir,self.q_std_vector_filename))\n",
    "        self.corpus_sentence_embeddings_dict_sec=dict(list(zip(self.q_std_list,self.std_sentence_embeddings)))\n",
    "        ## load model\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = SentenceTransformer(self.model_dir,device=device)\n",
    "        ## \n",
    "        \n",
    "    def cos_sim(self,vector_a,vector_b):\n",
    "        vector_a = np.mat(vector_a)\n",
    "        vector_b = np.mat(vector_b)\n",
    "        num = float(vector_a * vector_b.T)\n",
    "        denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "        cos = num / denom\n",
    "        sim = 0.5 + 0.5 * cos\n",
    "        return sim\n",
    "    \n",
    "    def cos_sim4matrix(self,arr, brr):\n",
    "        return 0.5 + 0.5 * (arr.dot(brr.T) / (np.sqrt(np.sum(arr * arr)) * np.sqrt(np.sum(brr * brr, axis = 1))))\n",
    "    def cos_sim4matrix_2(self,arr, brr):\n",
    "        return (arr.dot(brr.T) / (np.sqrt(np.sum(arr * arr)) * np.sqrt(np.sum(brr * brr, axis = 1))))\n",
    "    \n",
    "    def get_topK(self,text,K=20):\n",
    "        text_embedding=self.model.encode([text])[0]\n",
    "#         sims_with_std=np.array([self.cos_sim(vec,text_embedding) for vec in self.std_sentence_embeddings])\n",
    "        sims_with_std=np.array(self.cos_sim4matrix_2(text_embedding,self.std_sentence_embeddings))\n",
    "        sort_idx=np.argsort(-sims_with_std)[:K]\n",
    "        sims_q_sort=[self.q_std_list[idx] for idx in sort_idx]\n",
    "        sims_values=[sims_with_std[idx] for idx in sort_idx]\n",
    "        result=list(zip(sims_q_sort,sims_values))\n",
    "        return(result)\n",
    " \n",
    "\n",
    "set_model_gpu(setgpu=True) \n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "def pytorch_cos_sim(a: Tensor, b: Tensor):\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = a / a.norm(dim=1)[:, None]\n",
    "    b_norm = b / b.norm(dim=1)[:, None]\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "\n",
    "\n",
    "get_sim_step1=get_sim_first(config.fst_model_dir,config.fst_data_dir,config.fst_q_std_vectors_file)\n",
    "get_sim_step2=get_sim_second(config.sec_model_dir,config.sec_data_dir,config.sec_q_std_vectors_file)\n",
    "\n",
    "print('标准问数量',len(get_sim_step1.q_std_list),len(get_sim_step2.q_std_list))\n",
    "\n",
    "def get_sec_result(text,std_texts):\n",
    "    a_text_embeddings=x_texts_embeddings_dict_2[text]\n",
    "    b_text_embeddings=np.array([get_sim_step2.corpus_sentence_embeddings_dict_sec[v] for v in std_texts])\n",
    "#     print(a_text_embeddings.shape,b_text_embeddings.shape)\n",
    "    sims_with_std=cos_sim4matrix_2(a_text_embeddings,b_text_embeddings).reshape(-1)\n",
    "    sort_idx=np.argsort(-sims_with_std).tolist()\n",
    "    intents_sort=[std_texts[idx] for idx in sort_idx]\n",
    "    sims_values=[sims_with_std[idx] for idx in sort_idx]\n",
    "    result=list(zip(intents_sort,sims_values))\n",
    "    return(result)\n",
    "# get_sec_result(text,std_texts)\n",
    "\n",
    "\n",
    "\n",
    "# 第一阶段\n",
    "time1=datetime.datetime.now();print(time1)\n",
    "print('************',text)\n",
    "result_first=get_sim_step1.get_topK(text=text,K=10)\n",
    "print('第一阶段\\n',result_first[0:20])\n",
    "first_intents=[v[0] for v in result_first]\n",
    "a_texts_embeddings_2=np.array(get_sim_step2.model.encode([text]))\n",
    "b_texts_embeddings_2=np.array([get_sim_step2.corpus_sentence_embeddings_dict_sec[v] for v in first_intents])\n",
    "sims_with_std=get_sim_step2.cos_sim4matrix_2(a_texts_embeddings_2,b_texts_embeddings_2).reshape(-1)\n",
    "sort_idx=np.argsort(-sims_with_std).tolist()\n",
    "intents_sort=[first_intents[idx] for idx in sort_idx]\n",
    "sims_values=[sims_with_std[idx] for idx in sort_idx]\n",
    "result_second=list(zip(intents_sort,sims_values))\n",
    "\n",
    "time2=datetime.datetime.now();print(time2)\n",
    "time_diff=time2-time1;print('cost time:',time_diff.seconds)\n",
    "\n",
    "print('*************************')\n",
    "print('第二阶段\\n',result_second[0:20])\n",
    "\n",
    "# # x='电脑版官方网站'\n",
    "# y='钠离子电池对行业的影响'\n",
    "# print(get_sim_step2.cos_sim4matrix_2(np.array(get_sim_step1.model.encode([text])),np.array(get_sim_step1.model.encode([y]))))\n",
    "# # print('x',np.array(get_sim_step2.model.encode([x]))\n",
    "# # print('y',np.array(get_sim_step2.model.encode([y]))\n",
    "# # print(get_sim_step2.cos_sim4matrix_2(np.array(get_sim_step2.model.encode([x])),np.array(get_sim_step2.model.encode([y]))))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}